# 104 職缺爬蟲程式

這個專案是一個用 Python 撰寫的網路爬蟲，主要用來爬取 104 人力銀行上符合關鍵字的職缺資訊。程式採用 Selenium 加上 ChromeDriver 來模擬瀏覽器操作，並整合 Supabase 作為資料上傳後端。該程式同時實現了詳細資訊解析，包括應徵資訊、工作條件、福利、聯絡方式等資料的擷取，以及自動儲存資料到本機 JSON 檔案與雲端資料庫的功能。

## 功能特色

- **自動化爬取職缺資訊：** 利用 Selenium 控制瀏覽器，自動滾動並收集職缺列表。
- **詳細頁面擷取：** 進入各個職缺詳情頁面，擷取標題、更新日期、公司資訊、職缺描述、應徵人數、福利、工具與技能分佈等資料。
- **資料儲存與上傳：** 將擷取資料儲存到 JSON 檔案，並依指定條件批次上傳資料到 Supabase 資料庫。
- **錯誤處理機制：** 內建錯誤計數與日誌紀錄，當爬蟲出現錯誤時可中斷或繼續嘗試處理剩餘職缺。
- **分關鍵字爬蟲：** 支援以多個關鍵字進行職缺搜尋與擷取，可自由定義關鍵字清單。

## 前置需求

在執行此爬蟲前，請確認您的環境中已安裝以下套件：
- Python 3.7 或以上版本
- Selenium
- webdriver-manager
- fake-useragent
- supabase-py
- 其他依賴項：`requests`、`logging`、`json`

此外，請留意以下幾點設定與注意事項：
- 在 `crawl_jobs()` 函式中，參數 `max_scrolls` 可用來設定 104 搜尋頁面要往下滑動尋找職缺的次數。  
- 若只想處理部分職缺，可以將爬取職缺列表的迴圈  
  `for job in current_jobs`  
  修改成  
  `for job in current_jobs[:5]`  
  來只抓取前五個職缺。
- Supabase 連線設定部分，請將  
  `supabase_url: str = "url"`  
  與  
  `supabase_key: str = "key"`  
  修改為您所屬組織的 URL 與金鑰，以確保資料能正確上傳至雲端資料庫。
- 若要使用 `line_request()` 功能，請填上正確的使用者代碼與金鑰。此功能利用 `requests.request()` 傳送訊息給 Line 使用者，但請注意免費官方帳號每個月僅有限額 200 則訊息發送。

## 使用流程

1. **設定 ChromeDriver 與瀏覽器選項：**  
   程式中會根據執行環境自動調整 Chrome 的執行路徑與選項（例如 Docker 與 Windows 環境的差異處理）。

2. **設定日誌紀錄：**  
   程式開始時會呼叫 `setup_logging()`，自動建立日誌目錄並產生日誌檔案，以方便後續調試。

3. **爬蟲邏輯與頁面處理：**  
   - 利用 Selenium 模擬滾動頁面，直到新職缺不再載入或達到最大滾動次數。  
   - 從列表頁面點擊職缺連結，進入詳細職缺頁面進行資料擷取。  
   - 程式中亦透過開啟新的分頁來獲取進一步的應徵資訊（如教育程度、性別分佈、語言能力等）。

4. **資料儲存與上傳：**  
   利用 `x_save()` 函式，每處理一定數量的資料後，程式會自動將資料儲存成 JSON 檔案至指定資料夾，同時根據資料表名稱將資料上傳至 Supabase。

5. **錯誤處理：**  
   - 當錯誤次數達到預設上限時，程式將暫停處理該關鍵字並將未處理的職缺儲存至 `undo` 目錄。  
   - 所有執行步驟與錯誤資訊都將記錄於日誌檔，方便後續排查及修正。

## 如何使用

1. **修改關鍵字清單：**  
   根據需求修改 `keyword_list` 清單，例如：
   ````python
   keyword_list = ["系統工程師", "網路管理工程師", "資安工程師", "資訊設備管制人員", "雲端工程師", "網路安全分析師"]
